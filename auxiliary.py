import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque

class StateRepresentationTask(nn.Module):
    """
    Decoder-only State Representation (SR) auxiliary task.
    Reconstructs observations from features generated by the main network's shared layers.
    """
    def __init__(self, feature_dim, obs_dim, hidden_dim=128, device="cpu"):
        super(StateRepresentationTask, self).__init__()
        self.device = device
        self.feature_dim = feature_dim  # Dimension of features from main model
        self.obs_dim = obs_dim          # Dimension of the original observation to reconstruct
        self.hidden_dim = hidden_dim

        # Decoder: reconstruct observation from features
        self.decoder = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim*2),
            nn.LayerNorm(hidden_dim*2),
            nn.ReLU(),
            nn.Linear(hidden_dim*2, obs_dim)
        )

        self.to(self.device)

    def forward(self, features):
        # Decode features back to observation
        reconstruction = self.decoder(features)
        return reconstruction

    def get_loss(self, features, original_observations):
        # Get reconstruction from features
        reconstruction = self.forward(features)
        # Calculate smooth L1 loss between reconstruction and original observation
        loss = F.smooth_l1_loss(reconstruction, original_observations)
        return loss


class RewardPredictionTask(nn.Module):
    """
    LSTM-based network for Reward Prediction (RP) auxiliary task.
    Predicts immediate rewards based on a sequence of features from the main model.
    Adjusts reward classification thresholds dynamically.
    """
    def __init__(self, input_dim, hidden_dim=64, sequence_length=20, device="cpu", debug=False):
        super(RewardPredictionTask, self).__init__()
        self.device = device
        self.hidden_dim = hidden_dim
        self.sequence_length = sequence_length
        self.debug = debug
        self.input_dim = input_dim  # Store the expected input dimension (feature dim)

        # Projection layer to handle varying input dimensions
        self.projection = nn.Linear(input_dim, hidden_dim)

        # LSTM layer to process feature sequences
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)

        # Output layer for 3-class classification (negative, near-zero, positive reward)
        self.output_layer = nn.Linear(hidden_dim, 3)

        # Thresholds for classifying rewards - initial values, will be adjusted
        self.pos_threshold = 0.01  # Start slightly wider than default
        self.neg_threshold = -0.01 # Start slightly wider than default

        # Internal buffer for recent rewards used for threshold adjustment
        self.threshold_reward_buffer = deque(maxlen=500) # Store recent rewards
        self.threshold_adjust_freq = 500 # How often (in rewards seen) to adjust thresholds
        self.threshold_min_buffer_size = 100 # Min rewards needed before first adjustment
        self.threshold_momentum = 0.95 # Momentum for threshold updates
        self.min_threshold_separation = 0.001 # Minimum gap between pos and neg thresholds

        # Debug counters
        self.debug_reward_counts = {"negative": 0, "zero": 0, "positive": 0}
        self.debug_total_rewards = 0

        self.to(self.device)

    def _adjust_thresholds(self):
        """Dynamically adjust reward thresholds based on recent reward history."""
        if len(self.threshold_reward_buffer) < self.threshold_min_buffer_size:
            return # Not enough data yet

        # Convert buffer to tensor for calculations
        rewards_tensor = torch.tensor(list(self.threshold_reward_buffer), dtype=torch.float32)

        # Calculate percentiles for a balanced 3-way split (e.g., 33rd and 67th)
        try:
            # Using quantile directly might be more robust for larger tensors
            # Ensure the tensor is not empty
            if rewards_tensor.numel() == 0:
                 if self.debug: print("[RP DEBUG Adjust] Reward buffer empty, cannot adjust thresholds.")
                 return

            q_neg = torch.quantile(rewards_tensor, 0.33)
            q_pos = torch.quantile(rewards_tensor, 0.67)

            new_neg_threshold = q_neg.item()
            new_pos_threshold = q_pos.item()

        except Exception as e:
             if self.debug: print(f"[RP DEBUG Adjust] Error calculating quantiles: {e}")
             # Fallback to simple min/max if quantiles fail (less robust)
             if rewards_tensor.numel() > 0:
                 min_val = torch.min(rewards_tensor).item()
                 max_val = torch.max(rewards_tensor).item()
                 range_val = max_val - min_val
                 new_neg_threshold = min_val + 0.33 * range_val
                 new_pos_threshold = min_val + 0.67 * range_val
             else:
                 # Cannot proceed if buffer is empty and quantiles failed
                 return


        # Ensure minimum separation between thresholds
        if new_pos_threshold - new_neg_threshold < self.min_threshold_separation:
            mid_point = (new_pos_threshold + new_neg_threshold) / 2
            new_neg_threshold = mid_point - self.min_threshold_separation / 2
            new_pos_threshold = mid_point + self.min_threshold_separation / 2
            # Ensure neg is still <= pos after adjustment
            if new_neg_threshold > new_pos_threshold:
                 new_neg_threshold = new_pos_threshold # Safety catch

        # Update thresholds with momentum
        self.neg_threshold = self.threshold_momentum * self.neg_threshold + (1 - self.threshold_momentum) * new_neg_threshold
        self.pos_threshold = self.threshold_momentum * self.pos_threshold + (1 - self.threshold_momentum) * new_pos_threshold

        # Final check: ensure neg <= pos
        if self.neg_threshold > self.pos_threshold:
            # If they crossed, reset them around the average with separation
            avg_threshold = (self.neg_threshold + self.pos_threshold) / 2
            self.neg_threshold = avg_threshold - self.min_threshold_separation / 2
            self.pos_threshold = avg_threshold + self.min_threshold_separation / 2


        if self.debug:
            print(f"[RP DEBUG Adjust] Adjusted thresholds - Neg: {self.neg_threshold:.6f}, Pos: {self.pos_threshold:.6f} (Buffer size: {len(self.threshold_reward_buffer)})")


    def forward(self, x_seq):
        # x_seq shape: [batch_size, sequence_length, input_dim]
        batch_size, seq_len, actual_dim = x_seq.shape

        # Check if input dimensions match the expected dimensions
        # If not, recreate the projection layer with the correct dimensions
        if actual_dim != self.input_dim:
            if self.debug:
                print(f"[RP DEBUG] Input dimension mismatch. Expected: {self.input_dim}, Got: {actual_dim}. Recreating projection layer.")

            # Create a new projection layer with correct input dimensions
            self.projection = nn.Linear(actual_dim, self.hidden_dim).to(self.device)
            # Update stored input dimension
            self.input_dim = actual_dim

        # Reshape to process all sequence elements at once
        x_reshaped = x_seq.reshape(batch_size * seq_len, -1)

        # Now apply the projection
        projected = self.projection(x_reshaped)

        # Reshape back to sequence format
        projected = projected.reshape(batch_size, seq_len, self.hidden_dim)

        # Now pass through LSTM
        lstm_out, _ = self.lstm(projected)

        # Take only the last timestep's output
        last_output = lstm_out[:, -1, :]

        # Predict reward class
        logits = self.output_layer(last_output)
        return logits

    def get_loss(self, x_seq, rewards):
        # Get reward class predictions
        logits = self.forward(x_seq)

        # Convert rewards to class labels:
        # 0: negative, 1: near-zero, 2: positive
        labels = torch.zeros_like(rewards, dtype=torch.long, device=self.device)
        labels[rewards > self.pos_threshold] = 2
        labels[rewards < self.neg_threshold] = 0
        labels[(rewards >= self.neg_threshold) & (rewards <= self.pos_threshold)] = 1

        # Calculate class distributions for debugging
        self.debug_total_rewards += len(rewards)
        neg_count = int((labels == 0).sum().item())
        zero_count = int((labels == 1).sum().item())
        pos_count = int((labels == 2).sum().item())

        self.debug_reward_counts["negative"] += neg_count
        self.debug_reward_counts["zero"] += zero_count
        self.debug_reward_counts["positive"] += pos_count

        # Update internal reward buffer for threshold adjustment
        self.threshold_reward_buffer.extend(rewards.cpu().tolist())

        # Periodically adjust thresholds based on buffer
        if self.debug_total_rewards % self.threshold_adjust_freq == 0:
             self._adjust_thresholds()

        # Print detailed debug info periodically only if debug mode is enabled
        # Link debug print frequency to threshold adjustment frequency for tidiness
        if self.debug and self.debug_total_rewards % self.threshold_adjust_freq == 0:
            total = self.debug_reward_counts["negative"] + self.debug_reward_counts["zero"] + self.debug_reward_counts["positive"]
            neg_pct = 100 * self.debug_reward_counts["negative"] / max(1, total)
            zero_pct = 100 * self.debug_reward_counts["zero"] / max(1, total)
            pos_pct = 100 * self.debug_reward_counts["positive"] / max(1, total)

            print(f"[RP DEBUG] Reward class distribution after {self.debug_total_rewards} rewards:")
            print(f"[RP DEBUG]   Negative: {self.debug_reward_counts['negative']} ({neg_pct:.1f}%)")
            print(f"[RP DEBUG]   Zero: {self.debug_reward_counts['zero']} ({zero_pct:.1f}%)")
            print(f"[RP DEBUG]   Positive: {self.debug_reward_counts['positive']} ({pos_pct:.1f}%)")
            print(f"[RP DEBUG] Current batch - Neg: {neg_count}, Zero: {zero_count}, Pos: {pos_count}")
            print(f"[RP DEBUG] Current batch rewards sample: {rewards[:5].cpu().tolist()}")
            print(f"[RP DEBUG] Current thresholds - Pos: {self.pos_threshold:.6f}, Neg: {self.neg_threshold:.6f}") # Use more precision

            # Show statistics on threshold adjustment buffer rewards
            if len(self.threshold_reward_buffer) > 0:
                rewards_view = list(self.threshold_reward_buffer) # Get a snapshot
                min_reward = min(rewards_view)
                max_reward = max(rewards_view)
                avg_reward = sum(rewards_view) / len(rewards_view)
                print(f"[RP DEBUG] Threshold buffer stats ({len(rewards_view)} samples) - Min: {min_reward:.6f}, Max: {max_reward:.6f}, Avg: {avg_reward:.6f}")
            else:
                 print("[RP DEBUG] Threshold buffer stats - Buffer empty")

            # Check if class imbalance is severe (>95% in one class)
            if neg_pct > 95 or zero_pct > 95 or pos_pct > 95:
                print("[RP DEBUG] WARNING: Severe class imbalance detected in reward classes!")

                # Note: Suggestions are now less critical as thresholds adjust automatically
                # Still, printing the imbalance warning is useful.

        # Calculate cross-entropy loss
        loss = F.cross_entropy(logits, labels)

        # Debug prediction quality only if debug mode is enabled
        if self.debug:
            with torch.no_grad():
                predictions = torch.argmax(logits, dim=1)
                accuracy = (predictions == labels).float().mean().item()
                if self.debug_total_rewards % self.threshold_adjust_freq == 0: # Match freq
                    print(f"[RP DEBUG] Prediction accuracy: {accuracy:.4f}")

                    # Check if all predictions are the same class
                    pred_classes = predictions.unique()
                    if len(pred_classes) == 1:
                        print(f"[RP DEBUG] WARNING: All predictions are class {pred_classes.item()}!")

                    # Get confidence scores
                    softmax_probs = F.softmax(logits, dim=1)
                    avg_confidence = softmax_probs.max(dim=1)[0].mean().detach().item()
                    print(f"[RP DEBUG] Average prediction confidence: {avg_confidence:.4f}")
                    # print(f"[RP DEBUG] Logits sample: {logits[0].detach().cpu().tolist()}") # Maybe too verbose now

        return loss


class AuxiliaryTaskManager:
    """
    Manages auxiliary tasks to improve feature learning in the main model.
    Auxiliary task losses flow back through the main model's shared layers to improve representation learning.
    """
    def __init__(self, actor, obs_dim, sr_weight=1.0, rp_weight=1.0,
                 sr_hidden_dim=128, rp_hidden_dim=64, rp_sequence_length=5,
                 device="cpu", use_amp=False, update_frequency=8,
                 learning_mode="batch", debug=False, batch_size=4096,
                 internal_aux_batch_size=4096):
        """
        Initialize the auxiliary task manager

        Args:
            actor: The actor network (for feature extraction)
            obs_dim: Dimension of the observation space
            sr_weight: Weight for the state representation task
            rp_weight: Weight for the reward prediction task
            sr_hidden_dim: Hidden dimension for state representation decoder
            rp_hidden_dim: Hidden dimension for reward prediction
            rp_sequence_length: Sequence length for reward prediction
            device: Device to use for computation
            update_frequency: How often to update auxiliary tasks
            learning_mode: Either "batch" (for PPO) or "stream" (for StreamAC)
            batch_size: Batch size for sampling from history (used by compute_losses)
            internal_aux_batch_size: Size of mini-batches for processing within compute_losses_for_batch
        """
        self.device = device
        self.internal_aux_batch_size = internal_aux_batch_size
        self.sr_weight = sr_weight
        self.rp_weight = rp_weight
        self.rp_sequence_length = rp_sequence_length
        self.actor = actor
        self.debug = debug
        self.use_amp = use_amp  # Store use_amp parameter for compatibility
        self.learning_mode = learning_mode
        self.update_frequency = update_frequency
        self.update_counter = 0
        self.batch_size = batch_size
        self.obs_dim = obs_dim  # Store observation dimension

        # Get feature dimension from actor (for SR and RP tasks)
        # Try to infer feature_dim more robustly
        try:
            # Attempt to get hidden_dim attribute
            self.feature_dim = getattr(actor, 'hidden_dim')
        except AttributeError:
            # Fallback: Try to infer from the last linear layer's output size before the final action head
            last_linear_out = None
            for module in reversed(list(actor.modules())):
                if isinstance(module, nn.Linear):
                    # Check if it's likely the layer before the final output
                    # This is heuristic and might need adjustment based on model structure
                    if module.out_features != getattr(actor, 'action_shape', -1): # Assuming action_shape exists
                        last_linear_out = module.out_features
                        break
            if last_linear_out:
                self.feature_dim = last_linear_out
            else:
                # Final fallback if inference fails
                self.feature_dim = 512 # Default fallback
                if self.debug:
                    print("[AUX WARNING] Could not reliably infer feature_dim from actor. Using default 512.")


        if self.debug:
            print(f"[AUX INIT] Initializing AuxiliaryTaskManager with sr_weight={sr_weight}, rp_weight={rp_weight}")
            print(f"[AUX INIT] Device: {device}, Learning mode: {learning_mode}")
            print(f"[AUX INIT] Observation dimension: {obs_dim}, Feature dimension: {self.feature_dim}")
            print(f"[AUX INIT] RP Sequence Length: {rp_sequence_length}, Batch Size: {batch_size}, Internal Aux Batch: {internal_aux_batch_size}")

        # State representation task (decoder only, uses actor features as encoder)
        self.sr_task = StateRepresentationTask(
            feature_dim=self.feature_dim,
            obs_dim=obs_dim,
            hidden_dim=sr_hidden_dim,
            device=device
        )

        # Reward prediction task
        self.rp_task = RewardPredictionTask(
            input_dim=self.feature_dim,  # Uses actor features
            hidden_dim=rp_hidden_dim,
            sequence_length=rp_sequence_length,
            device=device,
            debug=debug # Pass debug flag
        )

        # Store references for torch.compile
        self.sr_model = self.sr_task
        self.rp_model = self.rp_task

        # History buffers - only store observations and rewards, not features
        self.batch_history_maxlen = 5000
        self.stream_history_maxlen = 500
        history_maxlen = self.batch_history_maxlen if learning_mode == "batch" else self.stream_history_maxlen

        # Use a small maxlen for test cases to match expected test behavior
        if hasattr(actor, '_is_test') and actor._is_test:
            history_maxlen = rp_sequence_length

        self.obs_history = deque(maxlen=history_maxlen)
        self.reward_history = deque(maxlen=history_maxlen)

        # Counters and last loss values (for compatibility)
        self.update_count = 0
        self.last_sr_loss = 0.0
        self.last_rp_loss = 0.0
        self.history_size = 0

    @property
    def history_filled(self):
        """Property to track how many items are in the history (for test compatibility)"""
        return self.history_size

    @property
    def training(self):
        """Returns if the manager is in training mode (based on actor)"""
        return self.actor.training

    def update(self, observations, rewards, features=None):
        """
        Add new experiences to the history buffers. Does NOT compute losses.
        Features parameter is kept for compatibility but ignored since features
        will be generated on-the-fly during loss computation.

        Args:
            observations: New observations (Tensor or np.array) [batch_size, obs_dim] or [obs_dim]
            rewards: New rewards (Tensor or np.array) [batch_size] or scalar
            features: Ignored (kept for compatibility)
        """
        # Validate reward dimensions - should be scalar or 1D tensor/array
        if isinstance(rewards, torch.Tensor) and rewards.dim() > 1:
            raise ValueError(f"Rewards must be scalar or 1D tensor, got shape {rewards.shape}")
        elif isinstance(rewards, np.ndarray) and rewards.ndim > 1:
            raise ValueError(f"Rewards must be scalar or 1D array, got shape {rewards.shape}")

        # Ensure inputs are torch tensors on CPU (history stored on CPU)
        if isinstance(observations, torch.Tensor):
            obs_cpu = observations.detach().cpu()
        else:
            obs_cpu = torch.tensor(observations, dtype=torch.float32)

        if isinstance(rewards, torch.Tensor):
            rew_cpu = rewards.detach().cpu()
        else:
            rew_cpu = torch.tensor(rewards, dtype=torch.float32)

        # Handle single vs batch inputs
        if obs_cpu.dim() == 1:  # Single observation
            obs_cpu = obs_cpu.unsqueeze(0)
            rew_cpu = rew_cpu.unsqueeze(0) if rew_cpu.dim() == 0 else rew_cpu

        # Add experiences to history
        num_added = 0
        for i in range(obs_cpu.shape[0]):
            self.obs_history.append(obs_cpu[i])
            self.reward_history.append(rew_cpu[i])
            num_added += 1

        self.history_size = len(self.obs_history)  # Update history size

        # Update counter for test compatibility (though less relevant now)
        # self.update_counter += 1
        # if self.update_counter >= self.update_frequency:
        #     self.update_counter = 0 # Reset when we reach the update frequency

        return {"items_added": num_added}

    def compute_losses(self):
        """
        DEPRECATED for PPO. Use compute_losses_for_batch instead.
        Compute auxiliary task losses by sampling from the history buffer.
        This is kept for potential use with stream-based algorithms.

        Returns:
            dict: Dictionary containing SR and RP loss tensors (not detached)
        """
        if self.learning_mode == "batch":
             if self.debug:
                 print("[AUX WARNING] compute_losses called in batch mode. Use compute_losses_for_batch for PPO.")
             # Simulate batch computation by sampling
             if self.history_size >= self.batch_size:
                 indices = np.random.choice(self.history_size, self.batch_size, replace=False)
                 obs_batch = torch.stack([self.obs_history[i] for i in indices]).to(self.device)
                 rewards_batch = torch.stack([self.reward_history[i] for i in indices]).to(self.device)
                 # Note: This bypasses the internal mini-batching of compute_losses_for_batch
                 # It assumes the sampled self.batch_size is small enough.
                 return self.compute_losses_for_batch(obs_batch, rewards_batch)
             else:
                 return {"sr_loss": torch.tensor(0.0, device=self.device),
                         "rp_loss": torch.tensor(0.0, device=self.device),
                         "sr_loss_scalar": 0.0, "rp_loss_scalar": 0.0}

        # --- Logic for stream mode (if needed) ---
        # This part might need adjustments based on how stream algorithms handle aux tasks
        sr_loss = torch.tensor(0.0, device=self.device)
        rp_loss = torch.tensor(0.0, device=self.device)
        sr_loss_scalar = 0.0
        rp_loss_scalar = 0.0

        # SR Task (using most recent observation)
        if self.sr_weight > 0 and self.history_size > 0:
            obs_latest = self.obs_history[-1].unsqueeze(0).to(self.device) # Add batch dim
            with torch.set_grad_enabled(self.training):
                try:
                    _, features_latest = self.actor(obs_latest, return_features=True)
                    sr_loss = self.sr_task.get_loss(features_latest, obs_latest) * self.sr_weight
                    sr_loss_scalar = sr_loss.detach().item()
                except Exception as e:
                    if self.debug: print(f"[AUX DEBUG Stream SR] Error: {e}")

        # RP Task (using most recent sequence)
        if self.rp_weight > 0 and self.history_size >= self.rp_sequence_length:
            obs_seq_list = [self.obs_history[i] for i in range(self.history_size - self.rp_sequence_length, self.history_size)]
            reward_target = self.reward_history[-1]

            obs_seq_tensor = torch.stack(obs_seq_list).unsqueeze(0).to(self.device) # Add batch dim
            reward_target_tensor = reward_target.unsqueeze(0).to(self.device) # Add batch dim

            batch_size, seq_len, obs_dim = obs_seq_tensor.shape
            obs_flat = obs_seq_tensor.reshape(-1, obs_dim)

            with torch.set_grad_enabled(self.training):
                try:
                    _, features_flat = self.actor(obs_flat, return_features=True)
                    feature_dim = features_flat.shape[-1]
                    features_seq_tensor = features_flat.reshape(batch_size, seq_len, feature_dim)
                    rp_loss = self.rp_task.get_loss(features_seq_tensor, reward_target_tensor) * self.rp_weight
                    rp_loss_scalar = rp_loss.detach().item()
                except Exception as e:
                    if self.debug: print(f"[AUX DEBUG Stream RP] Error: {e}")

        self.last_sr_loss = sr_loss_scalar
        self.last_rp_loss = rp_loss_scalar
        self.update_count += 1 # Track compute_losses calls

        return {"sr_loss": sr_loss, "rp_loss": rp_loss,
                "sr_loss_scalar": sr_loss_scalar, "rp_loss_scalar": rp_loss_scalar}


    def compute_losses_for_batch(self, obs_batch, rewards_batch):
        """
        Compute auxiliary task losses for a specific batch of data using internal mini-batching.
        This is intended for use within PPO's update loop.

        Args:
            obs_batch: Tensor of observations for the batch [batch_size, obs_dim] (on CPU or GPU)
            rewards_batch: Tensor of rewards for the batch [batch_size] (on CPU or GPU)

        Returns:
            dict: Dictionary containing SR and RP loss tensors (not detached)
                  and their scalar equivalents for logging.
        """
        total_sr_loss = torch.tensor(0.0, device=self.device)
        total_rp_loss = torch.tensor(0.0, device=self.device)
        sr_loss_scalar = 0.0
        rp_loss_scalar = 0.0
        sr_processed_count = 0
        rp_processed_count = 0
        current_batch_size = obs_batch.shape[0]

        # Ensure rewards_batch is on the correct device (needed for RP target indexing)
        rewards_batch_device = rewards_batch.to(self.device)

        # --- State Representation Task (Internal Mini-batching) ---
        # Initialize SR loss tensor in case the loop doesn't run or sr_weight is 0
        sr_loss = torch.tensor(0.0, device=self.device)
        if self.sr_weight > 0:
            for i in range(0, current_batch_size, self.internal_aux_batch_size):
                mini_batch_end = min(i + self.internal_aux_batch_size, current_batch_size)
                obs_mini_batch = obs_batch[i:mini_batch_end].to(self.device) # Move mini-batch to device
                mini_batch_actual_size = obs_mini_batch.shape[0]

                if mini_batch_actual_size == 0: continue

                # Get features from actor WITH gradients for the mini-batch
                with torch.set_grad_enabled(self.training):
                    try:
                        # Ensure actor expects features to be returned
                        actor_output = self.actor(obs_mini_batch, return_features=True)
                        # Handle different actor output formats (e.g., tuple or single tensor)
                        if isinstance(actor_output, tuple):
                            _, features_mini_batch = actor_output
                        else:
                            # Assuming the actor might just return features directly if return_features=True
                            # This might need adjustment based on the specific actor implementation
                            features_mini_batch = actor_output
                            if self.debug: print("[AUX DEBUG Batch SR Mini] Warning: Actor output format might be unexpected (expected tuple).")

                    except (TypeError, ValueError, AttributeError, RuntimeError) as e:
                        if self.debug: print(f"[AUX DEBUG Batch SR Mini] Error getting features (batch {i // self.internal_aux_batch_size}): {e}")
                        features_mini_batch = None # Fallback

                # Compute SR loss if we have features
                if features_mini_batch is not None:
                    try:
                        sr_loss_mini = self.sr_task.get_loss(features_mini_batch, obs_mini_batch) * self.sr_weight
                        if not torch.isnan(sr_loss_mini) and not torch.isinf(sr_loss_mini):
                            # Accumulate loss weighted by mini-batch size
                            total_sr_loss += sr_loss_mini # Loss function usually averages; scale later
                            sr_processed_count += mini_batch_actual_size
                        else:
                            if self.debug: print("[AUX DEBUG Batch SR Mini] Invalid SR loss detected (NaN/Inf)")
                    except Exception as e:
                        if self.debug: print(f"[AUX DEBUG Batch SR Mini] Error calculating SR loss (batch {i // self.internal_aux_batch_size}): {e}")
                else:
                     if self.debug: print(f"[AUX DEBUG Batch SR Mini] Skipped loss calculation: No features obtained (batch {i // self.internal_aux_batch_size}).")

            # Average the loss over all processed items
            if sr_processed_count > 0:
                # Average the accumulated loss (assumes get_loss returns mean loss for the mini-batch)
                num_mini_batches = (sr_processed_count + self.internal_aux_batch_size - 1) // self.internal_aux_batch_size
                sr_loss = total_sr_loss / max(1, num_mini_batches) # Average over mini-batches
                sr_loss_scalar = sr_loss.detach().item()
            else:
                # sr_loss already initialized to 0 tensor
                sr_loss_scalar = 0.0
                if self.debug: print("[AUX DEBUG Batch SR] No items processed for SR task.")
        else: # sr_weight <= 0
            # sr_loss already initialized to 0 tensor
            sr_loss_scalar = 0.0

        # --- Reward Prediction Task (Internal Mini-batching) ---
        # Initialize RP variables before the conditional block
        rp_loss = torch.tensor(0.0, device=self.device)
        rp_loss_scalar = 0.0
        obs_dim_rp = None
        seq_len_rp = None

        # Construct sequences directly from the current batch data.
        if self.rp_weight > 0 and current_batch_size >= self.rp_sequence_length:
            obs_seq_batch = []
            reward_target_batch = []

            # Iterate through the original batch (on CPU or GPU) to define sequences
            for i in range(current_batch_size - self.rp_sequence_length + 1):
                obs_seq = obs_batch[i : i + self.rp_sequence_length]
                obs_seq_batch.append(obs_seq)
                target_reward_idx = i + self.rp_sequence_length - 1
                reward_target_batch.append(rewards_batch_device[target_reward_idx]) # Use rewards on device

            valid_sequences = len(obs_seq_batch)
            obs_seq_batch_tensor = None
            reward_targets_tensor = None

            if valid_sequences > 0:
                try:
                    # Stack sequences - keep on original device initially if possible
                    # This might still be large, but avoids immediate GPU OOM if obs_batch was on CPU
                    obs_seq_batch_tensor = torch.stack(obs_seq_batch)
                    reward_targets_tensor = torch.stack(reward_target_batch) # Already on device
                    obs_dim_rp = obs_seq_batch_tensor.shape[-1]
                    seq_len_rp = obs_seq_batch_tensor.shape[1]

                except RuntimeError as e:
                    if self.debug: print(f"[AUX DEBUG Batch RP] Error stacking sequences/rewards: {e}")
                    valid_sequences = 0 # Cannot proceed

                # Process sequences in mini-batches only if tensors were created successfully
                if valid_sequences > 0 and obs_seq_batch_tensor is not None and reward_targets_tensor is not None:
                    for i in range(0, valid_sequences, self.internal_aux_batch_size):
                        mini_batch_end = min(i + self.internal_aux_batch_size, valid_sequences)
                        obs_seq_mini_batch = obs_seq_batch_tensor[i:mini_batch_end].to(self.device) # Move mini-batch to device
                        reward_targets_mini_batch = reward_targets_tensor[i:mini_batch_end] # Already on device
                        mini_batch_actual_size = obs_seq_mini_batch.shape[0]

                        if mini_batch_actual_size == 0: continue

                        # Ensure obs_dim_rp and seq_len_rp were set
                        if obs_dim_rp is None or seq_len_rp is None:
                            if self.debug: print("[AUX DEBUG Batch RP Mini] Error: obs_dim_rp or seq_len_rp not set.")
                            break

                        obs_flat_rp_mini = obs_seq_mini_batch.reshape(-1, obs_dim_rp)

                        with torch.set_grad_enabled(self.training):
                            try:
                                # Ensure actor expects features to be returned
                                actor_output_rp = self.actor(obs_flat_rp_mini, return_features=True)
                                if isinstance(actor_output_rp, tuple):
                                     _, features_flat_rp_mini = actor_output_rp
                                else:
                                     features_flat_rp_mini = actor_output_rp # Fallback assumption
                                     if self.debug: print("[AUX DEBUG Batch RP Mini] Warning: Actor output format might be unexpected (expected tuple).")

                            except (TypeError, ValueError, AttributeError, RuntimeError) as e:
                                if self.debug: print(f"[AUX DEBUG Batch RP Mini] Error getting features (batch {i // self.internal_aux_batch_size}): {e}")
                                features_flat_rp_mini = None # Fallback

                        if features_flat_rp_mini is not None:
                            try:
                                feature_dim_rp = features_flat_rp_mini.shape[-1]
                                features_seq_tensor_rp_mini = features_flat_rp_mini.reshape(mini_batch_actual_size, seq_len_rp, feature_dim_rp)

                                rp_loss_mini = self.rp_task.get_loss(features_seq_tensor_rp_mini, reward_targets_mini_batch) * self.rp_weight

                                if not torch.isnan(rp_loss_mini) and not torch.isinf(rp_loss_mini):
                                    total_rp_loss += rp_loss_mini # Accumulate mean loss from mini-batch
                                    rp_processed_count += mini_batch_actual_size
                                else:
                                    if self.debug: print("[AUX DEBUG Batch RP Mini] Invalid RP loss detected (NaN/Inf)")
                            except Exception as e:
                                if self.debug: print(f"[AUX DEBUG Batch RP Mini] Error calculating RP loss (batch {i // self.internal_aux_batch_size}): {e}")
                        else:
                             if self.debug: print(f"[AUX DEBUG Batch RP Mini] Skipped loss calculation: No features obtained (batch {i // self.internal_aux_batch_size}).")

                    # Average the loss over all processed mini-batches
                    if rp_processed_count > 0:
                        num_mini_batches_rp = (rp_processed_count + self.internal_aux_batch_size - 1) // self.internal_aux_batch_size
                        rp_loss = total_rp_loss / max(1, num_mini_batches_rp) # Average over mini-batches
                        rp_loss_scalar = rp_loss.detach().item()
                    else:
                        # rp_loss already initialized to 0 tensor
                        rp_loss_scalar = 0.0
                        if self.debug: print("[AUX DEBUG Batch RP] No sequences processed for RP task.")
                else: # Failed to create tensors or valid_sequences was 0
                    if self.debug and valid_sequences > 0: print("[AUX DEBUG Batch RP] Skipped processing: Tensor creation failed.")
                    # rp_loss already initialized to 0 tensor
                    rp_loss_scalar = 0.0

            else: # valid_sequences == 0
                if self.debug: print("[AUX DEBUG Batch RP] Skipped: No valid sequences constructed from batch.")
                # rp_loss already initialized to 0 tensor
                rp_loss_scalar = 0.0
        else: # rp_weight <= 0 or not enough items
            if self.debug and self.rp_weight > 0:
                print(f"[AUX DEBUG Batch RP] Skipped: Not enough batch items ({current_batch_size}/{self.rp_sequence_length}) for sequence or RP weight is zero.")
            # rp_loss already initialized to 0 tensor
            rp_loss_scalar = 0.0


        # Store last computed scalar losses
        self.last_sr_loss = sr_loss_scalar
        self.last_rp_loss = rp_loss_scalar

        # Increment update count (tracks how many times losses were computed for this batch)
        self.update_count += 1

        # Return final averaged losses
        return {"sr_loss": sr_loss, "rp_loss": rp_loss,
                "sr_loss_scalar": sr_loss_scalar, "rp_loss_scalar": rp_loss_scalar}


    def reset(self):
        """Reset history buffers and task states"""
        # Clear history buffers
        self.obs_history.clear()
        self.reward_history.clear()
        self.history_size = 0  # Reset size tracker

        # Reset RP task specific state (like thresholds and buffer)
        if hasattr(self.rp_task, 'threshold_reward_buffer'):
            self.rp_task.threshold_reward_buffer.clear()
        if hasattr(self.rp_task, 'debug_reward_counts'):
            self.rp_task.debug_reward_counts = {"negative": 0, "zero": 0, "positive": 0}
            self.rp_task.debug_total_rewards = 0
            # Optionally reset thresholds to initial values or keep learned ones
            # self.rp_task.pos_threshold = 0.01
            # self.rp_task.neg_threshold = -0.01

        if self.debug:
            print("[AUX RESET] Reset called - cleared history buffers and RP task state")

    def reset_auxiliary_tasks(self):
        """Reset auxiliary task models entirely (re-initialize)"""
        if self.debug:
             print("[AUX RESET] Re-initializing auxiliary task models...")
        # Re-initialize SR task
        self.sr_task = StateRepresentationTask(
            feature_dim=self.feature_dim,
            obs_dim=self.obs_dim,
            hidden_dim=self.sr_task.hidden_dim, # Reuse existing hidden dim
            device=self.device
        )

        # Re-initialize RP task
        self.rp_task = RewardPredictionTask(
            input_dim=self.feature_dim,
            hidden_dim=self.rp_task.hidden_dim, # Reuse existing hidden dim
            sequence_length=self.rp_sequence_length,
            device=self.device,
            debug=self.debug
        )

        # Reset history buffers as well
        self.reset()

        # Reset manager counters
        self.update_counter = 0
        self.history_size = 0
        self.update_count = 0
        self.last_sr_loss = 0.0
        self.last_rp_loss = 0.0

        if self.debug:
            print("[AUX RESET] Complete reset of auxiliary task models and manager state")

    def set_learning_mode(self, mode):
        """
        Set the learning mode and adjust history buffer size accordingly.

        Args:
            mode: Either "batch" (for PPO) or "stream" (for StreamAC)
        """
        if mode not in ["batch", "stream"]:
            raise ValueError(f"Unknown learning mode: {mode}")

        prev_mode = self.learning_mode
        if prev_mode == mode:
            return # No change needed

        self.learning_mode = mode

        # Adjust buffer sizes
        if mode == "batch":
            new_maxlen = self.batch_history_maxlen
        else: # mode == "stream"
            new_maxlen = self.stream_history_maxlen

        # Create new deques with the correct maxlen and populate with existing data
        temp_obs = list(self.obs_history)
        temp_rewards = list(self.reward_history)

        self.obs_history = deque(maxlen=new_maxlen)
        self.reward_history = deque(maxlen=new_maxlen)

        # Extend with data (deque handles maxlen automatically)
        self.obs_history.extend(temp_obs)
        self.reward_history.extend(temp_rewards)

        self.history_size = len(self.obs_history)  # Update size tracker

        if self.debug:
            print(f"[AUX MODE] Changed learning mode from {prev_mode} to {mode}")
            print(f"[AUX MODE] New history maxlen: {new_maxlen}, Current history size: {self.history_size}")

    def get_state_dict(self):
        """Get state dict for saving task models and manager state"""
        state = {
            'sr_task': self.sr_task.state_dict(),
            'rp_task': self.rp_task.state_dict(),
            'sr_weight': self.sr_weight,
            'rp_weight': self.rp_weight,
            'history_size': self.history_size,
            'update_count': self.update_count,
            'last_sr_loss': self.last_sr_loss,
            'last_rp_loss': self.last_rp_loss,
            'learning_mode': self.learning_mode,
            # Include RP thresholds if dynamic adjustment is used
            'rp_pos_threshold': getattr(self.rp_task, 'pos_threshold', 0.01),
            'rp_neg_threshold': getattr(self.rp_task, 'neg_threshold', -0.01),
            # History buffers are usually not saved due to size
            # 'obs_history': list(self.obs_history),
            # 'reward_history': list(self.reward_history),
            # RP threshold buffer also usually not saved
            # 'rp_threshold_buffer': list(getattr(self.rp_task, 'threshold_reward_buffer', [])),
        }
        return state

    def load_state_dict(self, state_dict):
        """Load state dict for loading task models and manager state"""
        self.sr_task.load_state_dict(state_dict['sr_task'])
        self.rp_task.load_state_dict(state_dict['rp_task'])

        # Load other parameters
        self.sr_weight = state_dict.get('sr_weight', self.sr_weight)
        self.rp_weight = state_dict.get('rp_weight', self.rp_weight)
        # self.history_size = state_dict.get('history_size', 0) # History size will be determined by loaded buffers if they exist, otherwise 0
        self.update_count = state_dict.get('update_count', 0)
        self.last_sr_loss = state_dict.get('last_sr_loss', 0.0)
        self.last_rp_loss = state_dict.get('last_rp_loss', 0.0)
        self.learning_mode = state_dict.get('learning_mode', self.learning_mode)

        # Load RP thresholds if they exist in the state_dict
        if hasattr(self.rp_task, 'pos_threshold'):
            self.rp_task.pos_threshold = state_dict.get('rp_pos_threshold', self.rp_task.pos_threshold)
        if hasattr(self.rp_task, 'neg_threshold'):
            self.rp_task.neg_threshold = state_dict.get('rp_neg_threshold', self.rp_task.neg_threshold)

        # Adjust history buffer maxlen based on loaded learning mode
        self.set_learning_mode(self.learning_mode) # This adjusts maxlen

        # Load history buffers if saved (optional) - this example assumes they aren't saved/loaded
        # if 'obs_history' in state_dict:
        #     self.obs_history = deque(state_dict['obs_history'], maxlen=self.obs_history.maxlen)
        # if 'reward_history' in state_dict:
        #     self.reward_history = deque(state_dict['reward_history'], maxlen=self.reward_history.maxlen)
        self.history_size = len(self.obs_history) # Recalculate size after potential load

        # Load RP threshold buffer if saved (optional)
        # if 'rp_threshold_buffer' in state_dict and hasattr(self.rp_task, 'threshold_reward_buffer'):
             # self.rp_task.threshold_reward_buffer = deque(state_dict['rp_threshold_buffer'], maxlen=self.rp_task.threshold_reward_buffer.maxlen)

        if self.debug:
            print("[AUX LOAD] Loaded auxiliary task models and state from state dict.")
            print(f"[AUX LOAD] Loaded RP thresholds - Pos: {getattr(self.rp_task, 'pos_threshold', 'N/A'):.6f}, Neg: {getattr(self.rp_task, 'neg_threshold', 'N/A'):.6f}")
